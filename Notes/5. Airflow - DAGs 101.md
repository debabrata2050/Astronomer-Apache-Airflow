# ğŸŒ¬ï¸ Airflow â€” DAGs 101

## ğŸ§° Default Arguments in Airflow

- You can apply arguments to tasks like:  
  ğŸ” `retries`  
  â³ `execution_timeout`  
  ğŸ“§ `email_on_failure`, etc.

- ğŸ›‘ **Default behavior**:  
  Tasks have `retries=0` (fail immediately on error).

### ğŸ”„ Problem with Repetition

Manually setting `retries=3` on every task is repetitive:

```python
task_a = PythonOperator(task_id='task_a', python_callable=print_a, retries=3)
```

Doing this for every task? ğŸ˜© Not ideal.

âœ… Solution: `default_args`  
Avoid repetition by passing a default_args dictionary to the DAG:

```python
default_args = {
    'retries': 3,
}

@dag('my_dag',
     start_date=datetime(2025, 1, 1),
     default_args=default_args,
     description='A simple tutorial DAG',
     tags=['data_science'],
     schedule='@daily')
def my_dag():
    task_a = PythonOperator(task_id='task_a', python_callable=print_a)
    task_b = PythonOperator(task_id='task_b', python_callable=print_b)
    task_c = PythonOperator(task_id='task_c', python_callable=print_c)
    task_d = PythonOperator(task_id='task_d', python_callable=print_d)
    task_e = PythonOperator(task_id='task_e', python_callable=print_e)

    chain(task_a, [task_b, task_c], [task_d, task_e])
```
ğŸ‰ All tasks inherit retries=3 â€” no repetition needed.


## ğŸ“˜ DAG (Directed Acyclic Graph)

- ğŸ”‘ **Unique Identifier**  
  Every DAG must have a **unique `dag_id`**.

- ğŸ• **Start Date (Optional)**  
  Set using `start_date`. Defaults to `None` if not specified.

- â° **Schedule Interval (Optional)**  
  Defines how often the DAG should run  
  _Example_: `'@daily'`, `'0 12 * * *'`.

- ğŸ“ **Description & Tags**  
  Strongly recommended for:
  - Better documentation
  - Easier filtering in the Airflow UI

---

## âœ… Tasks in a DAG

- ğŸ”‘ **Unique Task ID**  
  Each task must have a **unique `task_id`** within its DAG.

- ğŸ§° **Where to Start**  
  Check out the [Astronomer Registry](https://registry.astronomer.io/) to explore available operators, sensors, and components.

- âš™ï¸ **Default Arguments (`default_args`)**  
  Apply shared parameters to multiple tasks using a dictionary.  
  _Common keys_: `start_date`, `retries`, `owner`, etc.

---

## ğŸ”— Defining Task Dependencies

â¡ï¸ **Bitshift Operators**  
  Use `>>` and `<<` to define task order.  
  ```python
  task1 >> task2  # task1 runs before task2
  ```
ğŸ“š **Lists of Tasks**  
  Define dependencies involving multiple tasks:
  ```python
  [task1, task2] >> task3

  ```
  
â›“ï¸ **`chain()` Utility Function**  
  Cleanly define complex chains:
  ```python
  from airflow.models.dag import chain
  chain(task1, [task2, task3], task4)
  ```

  ****

# ğŸ› ï¸ Practice: Creating your second data pipeline
## ğŸ“˜ Introduction  
In this activity, you will create a second data pipeline from scratch that:

- ğŸ“„ Create a file using the `BashOperator`  
- ğŸ Read the file using the `PythonOperator`

At the end of this activity, you will be able to:

- ğŸ“š Find the documentation you need to implement an operator  
- ğŸ’» Execute Bash commands from your DAG  
- ğŸ§© Create a DAG following best practices  
- ğŸ Execute Python functions from your DAG  
- ğŸ”— Define dependencies between your tasks


## âœ… Prerequisites  
- ğŸ–¥ï¸ Local development environment with the Astro CLI

## ğŸ§­ Instructions

### ğŸªœ Step 1: Defining the DAG

ğŸ“„ Create a dag file `check_dag.py`. Then, define a DAG with the identifier `check_dag`.

ğŸ•› We expect `check_dag` to run **every day at midnight** from the **1st of January 2025**.

ğŸ“ Also, the DAG should have the following description:  
`"DAG to check data"` and belongs to the `data_engineering` team.


### ğŸªœ Step 2: Creating the Tasks

We want to add **three tasks** to this DAG.

1. ğŸ“ The first task executes the following Bash command:
   ```bash
   echo "Hi there!" > /tmp/dummy
   ```
   âœ… This creates a file `dummy` in the `/tmp` directory with `"Hi there!"`.  
  ğŸ·ï¸ The task's name should be `create_file`  
  â• Use the `@task.bash` decorator for Bash commands.
2. ğŸ§ª The second task executes the following Bash command:
   ```bash
   test -f /tmp/dummy
   ```
   âœ… This verifies that the file `dummy` exists in the `/tmp` directory.  
   ğŸ·ï¸ The task's name should be `check_file`
3. ğŸ The third task executes the following Python function:
   ```python
   print(open('/tmp/dummy', 'rb').read())
   ```
   âœ… This reads and prints on the standard output the content of the `dummy` file.

### ğŸªœ Step 3: Defining the dependencies

ğŸ”— You should define the dependencies to get the order of execution:

```
create_file â†’ check_file â†’ read_file
```

ğŸ§ª Finally, make sure that you have no errors by going to the Airflow UI.  
ğŸ‘€ You should be able to see your DAG.

## ğŸ§¾ Final DAG Code: `check_dag.py`:

```python
from airflow.sdk import dag, task
from pendulum import datetime

@dag(
    schedule="@daily",
    start_date=datetime(2025, 1, 1),
    description="DAG to check data",
    tags=["data_engineering"],
)

def check_dag():

    @task.bash
    def create_file():
        return 'echo "Hi there!" >/tmp/dummy'

    @task.bash
    def check_file_exists():
        return 'test -f /tmp/dummy'
    
    @task
    def read_file():
        print(open('/tmp/dummy', 'rb').read())

    create_file() >> check_file_exists() >> read_file()

check_dag()
```
