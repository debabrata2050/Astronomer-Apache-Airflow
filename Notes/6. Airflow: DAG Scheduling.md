# 🌀 Airflow: DAG Scheduling

## 🚦 DAG Run States
- 🟡 **Queued** → Initial state  
- 🟢 **Running** → When the first task starts  
- ✅ **Success** / ❌ **Failure** → Final state, based on **leaf tasks**

## ⏰ Start Date & Scheduling Interval
- 📅 **Start Date**:  
  - Marks when the scheduler **starts backfilling**
  - Should be a **static timestamp** to avoid issues
- 🔁 **Schedule Interval**:  
  - Defines **how often** the DAG runs  
  - Can be set using **CRON**, `timedelta`, or a **custom timetable**

## 📆 Data Intervals & Logical Date
- Every DAG Run has:
  - 🟢 `data_interval_start`  
  - 🔴 `data_interval_end`  
  - 🧠 `logical_date` (usually equal to `data_interval_start`)

## 🧩 Catchup & Backfill
- ⚙️ **Airflow Config: `scheduler.catchup_by_default=False`** (default in recent versions)  
  - By default, the scheduler **only creates a DAG run for the latest interval** when a DAG is activated.  
  - Missed DAG runs from previous intervals are **not automatically created** unless explicitly enabled.
- 🔄 **`catchup=True` in DAG**  
  - Overrides the default config.  
  - Scheduler triggers **all missed DAG runs** from the `start_date` up to the latest interval (catchup enabled).  
  - Useful when you want to backfill and process historical data automatically.
- 🚫 **`catchup=False` in DAG**  
  - Prevents the scheduler from backfilling missed intervals, running **only the latest DAG run**.  
  - Recommended if your DAG is **not designed to handle backfill or catchup logic properly**.
- 🔙 **Backfilling via CLI**  
  - Allows manually running past or missed DAG runs regardless of `start_date` or catchup settings.  
  - Useful for ad-hoc reruns or historical data processing outside the scheduler’s normal operation.


## 💻 Airflow CLI
- Use CLI commands to:
  - 🔧 **Backfill** historical runs  
  - 🔁 Rerun specific data intervals  
  - ⚙️ Gain control over DAG execution beyond the scheduler
